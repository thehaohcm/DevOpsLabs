Preparing short-command before taking the exam:

alias ks="kubectl -n kube-system"

alias kgp="kubectl get pod"
alias kgd="kubectl get deploy"
alias kgs="kubectl get service"
alias kgn="kubectl get node"
alias kd="kubectl describe"
alias kge="kubectl get events --sort-by='.metadata.creationTimestamp' | tail -8"

export dy="--dry-run=client -oyaml"

export now="--force --grace-period 0"

echo "set ts=2 sts=2 sw=2 et" >> ~/.vimrc

create new user:
  kubectl config set-credentials [user_name] --client-certificate=[.crt_path] --client-key=[.key_path]

create new context:
  kubectl config set-context [context_name] --user=[user_name] --cluster=[cluster_name]

switch context:
  kubectl config use-context [context_name]

CSR:
  create new csr with yaml format
  kubectl certificate approve [csr_name] 
  create new role
  create new rolebinding

run temporary pod (and it will be removed after done) and run a command:
  kubectl run [pod_name] --image=[image_name] --rm -ti --command -- [command]
or kubectl -it  run busybox --rm --image=busybox -- sh
ex: kubectl run test --image=busybox --rm -ti --command -- sleep 4600


collect osImage item of all ndoes by using jsonpath:
  kubectl get nodes -o jsonpath="{.item[*].status.nodeInfo.osImage}"
or we can use jq tool to show json with pretty format:
  kubectl get nodes -o jsonpath | jq
otherwise: 
  kubectl get nodes -o json | jq -c 'path(..)|[.[]|tostring]|join(".")' | grep -i osImage | grep -v managedFields
install jq tool: 
  apt install jq

get resources in k8s cluster and sort by creation time:
  kubectl api-resources --sort-by='.metadata.creationTime'

create a new service account
kubectl create sa [service_account_name]

add serviceaccount for a pod (yaml file)
spec:
  serviceAccount: [service_account_name]
  
create role:
  kubectl create role [role_name] [-n [namespace]] --verb=[create,update,delete,...] --resource=[pod,configmaps,secrets]
create rolebinding:
  kubectl create rolebinding [role_binding_name] [-n [namespace]] --role=[role_name] [--serviceaccount=[namespace]:[serviceaccount_name]] [--user=[user_name]]
  
get all resource in k8s cluster:
  kubectl api-resources
  
check network plugin info:
  find /etc/cni/net.d/
  then read a file inside this folder

create a secret:
  kubectl create secret generic [secret_name] --from-literal=[key1]=[value1] --from-literal=[key2]=[value2]...
  
get cluster cidr network (2 ways):
  - kubectl cluster-info dump | grep -i 'cluster-ip-range'
  - cat /etc/kubernetes/manifest/kube-apiserver.yaml | grep -i 'cluster-ip-range'
  
join a node worker into cluster:
  run this comment in the master node (ssh into master node):
    kubeadm token create --print-join-command
  ssh into the new one worker node:
    then copy the join command in the master node and past into the worker node
    
get cert info of k8s clusters (server, client cert...):
  openssl x509 -noout -text -in [path of the cert file]
   ex: openssl x509 -noout -text -in /etc/kubernetes/pki/server.crt | grep -i 'After' -A3
  kubeadm certs check-expiration
  
  client certificate: /etc/kubernetes/kubelet.conf
  
renew cert with command
  kubeadm certs renew [service]
  ex: kubeadm certs renew apiserver
  
livenessProbe: check if container is available and 
readinessProd: check if the app is ready to be used serve traffic

monitor resource in:
  - nodes: kubectl top nodes
  - pod: kubectl top pods
  - pod + pod's containers: kubectl top pods --containers
  - specifc pod + pods containers: kubectl top pods [pod_name] --containers
  
tips:
  if a question is asking for inspect to a container of pod:
    - find which node that the pod is running inside
    - ssh into the node
    - use 'crictl ps | grep [pod_name]' to check and get pod's container_id and info
      use 'crictl inspect [pod_ip]' to get info about the container
    (use crictl command as a docker command alter)
    
  if a question asking for create a deployment which only allow one pod will be run inside one node (include master node):
    - use affinity + podAntiAffinity + (topologyKey: kubernetes.io/hostname): to make sure no node has 2 the same pod running inside
    - use toleration to run a pod in the master node (copy a declaration in daemonset k8s doc)
  => this deployment will behave as a daemonset  
  
  how to specify a pod to run a specific node:
  1. use spec.nodeName
  2. use spec.nodeSelector (with node's label)
  3. use nodeAffinity
  4. use podAffinity 
  4. use podAntiAffinity + (topologyKey: kubernete.io/hostname)

linux stuff tips:
  find a text with insensitiy case:
    grep -i '[word]'
  find multi texts with insensity case:
    grep -i -e '^[word1]|[word2]'
  count line number:
    wc -l
  replace a word with another word:
    sed -i 's/[word1]/[word2]/g'
  check response of url:
    curl -s [url/pod_name/service_name/ip]:[port]
  get an absolute path of a file:
    readlink -f [file_name]
  get a specific column data:
    awk '{print $[column_num]}'
